import requests
import re
import os
from collections import defaultdict
import concurrent.futures
import time

# ===== å‚æ•°é…ç½® =====
target_channels = [
    'CCTV1 ç»¼åˆ[1920x1080]', 'CCTV2 è´¢ç»[1920x1080]', 'CCTV13 æ–°é—»[1920x1080]',
    'CCTVä¸–ç•Œåœ°ç†[1920x1080]', 'CCTVå¤®è§†æ–‡åŒ–ç²¾å“[1920x1080]',
    'CCTVé£äº‘è¶³çƒ[1920x1080]', 'CCTVé«˜å°”å¤«Â·ç½‘çƒ[1920x1080]', 'CCTVå¤®è§†å°çƒ[1920x1080]',
    'CCTVæ€€æ—§å‰§åœº[1920x1080]', 'åŠŸå¤«å°', 'é¾ç¥¥æ™‚ä»£',
	'æ¹–å—å«è§†[1920x1080]', 'æ¹–å—éƒ½å¸‚é«˜æ¸… 12M', 'æ·±åœ³éƒ½å¸‚é¢‘é“',
    'äºšæ´²æ­¦ä¾ ', 'Nowçˆ†è°·å°', 'Nowæ˜Ÿå½±å°',
    'Playbo', 'æ¬§ç¾è‰ºæœ¯', 'ç¾å›½å®¶åº­'
]

exclude_keywords = ['chinamobile', 'tvgslb', 'è´­ç‰©', 'ç†è´¢']
exclude_domains = ['kkk.jjjj.jiduo.me', 'www.freetv.top']  # å¯ä»¥è½»æ¾åœ¨æ­¤æ·»åŠ æ›´å¤šåŸŸå
timeout = 15
max_workers = 20

# ===== ç¡®è®¤å½“å‰ç›®å½•å¹¶å‡†å¤‡æ–‡ä»¶è·¯å¾„ =====
cwd = os.getcwd()
log_path = os.path.join(cwd, 'speed_tv.txt')
out_path = os.path.join(cwd, 'filtered_streams.txt')

# ===== åˆ é™¤æ—§æ–‡ä»¶ =====
if os.path.exists(out_path):
    os.remove(out_path)
if os.path.exists(log_path):
    os.remove(log_path)
print("âœ… å·²æ¸…ç†æ—§çš„è¾“å‡ºæ–‡ä»¶")

# ===== æ”¶é›†æºæ•°æ® =====
all_lines = []
try:
    with open('iptv_list3.txt', 'r', encoding='utf-8') as f:
        all_lines.extend(f.readlines())
    print("ğŸ“„ å·²è¯»å–æœ¬åœ° iptv_list3.txt")
except FileNotFoundError:
    print("âš ï¸ æœ¬åœ°æ–‡ä»¶æœªæ‰¾åˆ°")

remote_urls = [
    'https://raw.githubusercontent.com/luoye20230624/hndxzb/refs/heads/main/iptv_list.txt',
    'https://raw.githubusercontent.com/ngdikman/hksar/refs/heads/main/dianxin.txt',
    'https://raw.githubusercontent.com/lcq61871/iptvz/refs/heads/main/maotv.txt'
]

# æ‹‰å–è¿œç¨‹æ•°æ®
for url in remote_urls:
    try:
        print(f"ğŸŒ æ‹‰å–: {url}")
        r = requests.get(url, timeout=timeout)
        if r.status_code != 200:
            continue
        lines = r.text.splitlines()
        if url.endswith('.m3u'):
            for i in range(len(lines) - 1):
                if lines[i].startswith('#EXTINF') and lines[i + 1].startswith('http'):
                    match = re.search(r',(.+)', lines[i])
                    if match:
                        stream_url = lines[i + 1].strip()
                        # æ’é™¤ç‰¹å®šåŸŸå
                        if any(domain in stream_url for domain in exclude_domains):
                            continue
                        all_lines.append(f"{match.group(1).strip()} {stream_url}")
        else:
            for line in lines[2:]:
                if ',' in line:
                    parts = line.split(',')
                    if len(parts) == 2:
                        channel, stream = parts
                        # æ’é™¤ç‰¹å®šåŸŸå
                        if any(domain in stream for domain in exclude_domains):
                            continue
                        all_lines.append(f"{channel.strip()} {stream.strip()}")
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")

print(f"ğŸ“º è·å–é¢‘é“æºæ€»æ•°: {len(all_lines)}")

# ===== ç²¾ç¡®åŒ¹é…å¹¶æ•´ç† =====
target_set = set(name.lower() for name in target_channels)
grouped_streams = defaultdict(set)

for line in all_lines:
    line = line.strip()
    if not line or 'http' not in line:
        continue
    match = re.match(r'(.+?)\s+(https?://\S+)', line)
    if not match:
        continue
    channel_name = match.group(1).strip()
    stream_url = match.group(2).strip()
    if channel_name.lower() in target_set and not any(keyword in channel_name for keyword in exclude_keywords):
        grouped_streams[channel_name].add(stream_url)

# ===== æµ‹é€Ÿå¹¶é€‰æœ€å¿«çš„å‰5ä¸ªæº =====
def test_stream_speed(url, timeout=10):
    try:
        start = time.time()
        response = requests.get(url, timeout=timeout, stream=True)
        if response.status_code == 200:
            duration = time.time() - start
            return url, duration
    except requests.RequestException:
        pass
    return url, float('inf')

def get_fastest_urls(channel, urls, top_n=5):
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(test_stream_speed, url, timeout): url for url in urls}
        for future in concurrent.futures.as_completed(future_to_url):
            try:
                url, duration = future.result()
                if duration != float('inf'):
                    results.append((url, duration))
            except Exception as e:
                print(f"âŒ æµ‹é€Ÿå¤±è´¥: {e}")
                continue
    results.sort(key=lambda x: x[1])  # æ ¹æ®å»¶è¿Ÿæ’åº
    with open(log_path, 'a', encoding='utf-8') as log:
        if results:
            log.write(f"ã€{channel}ã€‘æµ‹é€ŸæˆåŠŸ {len(results[:top_n])} æ¡\n")
            for url, delay in results[:top_n]:
                log.write(f"  {delay:.2f}s  {url}\n")
        else:
            log.write(f"ã€{channel}ã€‘âš ï¸ æ— å¯ç”¨æº\n")
    print(f"âœ… {channel} | å†™å…¥ {len(results[:top_n])} æ¡æµ‹é€Ÿæ—¥å¿—")
    return [url for url, _ in results[:top_n]]

# ===== ç”Ÿæˆæœ€ç»ˆèŠ‚ç›®è¡¨ =====
final_streams = defaultdict(list)

for channel, urls in grouped_streams.items():
    print(f"âš™ï¸ æ­£åœ¨æµ‹é€Ÿ: {channel} å…± {len(urls)} æ¡é“¾æ¥")
    fastest = get_fastest_urls(channel, urls, top_n=5)
    final_streams[channel].extend(fastest)

# ===== å†™å…¥ filtered_streams.txt =====
with open(out_path, 'w', encoding='utf-8') as f:
    f.write("abcé¢‘é“,#genre#\n")
    for channel, urls in sorted(final_streams.items()):
        for url in urls:
            f.write(f"{channel}, {url}\n")

print("âœ… å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š")
print(f"ğŸ“„ {out_path}")
print(f"ğŸ“„ {log_path}")
